{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8bba355",
   "metadata": {},
   "source": [
    "___\n",
    "# <font color= #d4b1e6> **Laboratorio 2: Training Neural Networks** </font>\n",
    "- <Strong> Nombre de los integrantes: </Strong>  <font color=\"blue\">`Sarah Lucía Beltrán, Priscila Cervantes Ramírez, Mónica Ibarra Herrera & Antonia Horburger` </font>\n",
    "- <Strong> Materia: </Strong>  <font color=\"blue\">`Aprendizaje Máquina` </font>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda08dd1",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Abstracto** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f6a717",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Método** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a10d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ffa94d",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Reproducibilidad** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15414a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7225b",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Configuración inicial** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('./data')\n",
    "RESULTS_DIR = Path('./results_autorun')\n",
    "CHECKPOINTS_DIR = RESULTS_DIR/'checkpoints'\n",
    "HISTORY_DIR = RESULTS_DIR/'history'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HISTORY_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5192bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment hyperparams (reduce epochs for quick debugging)\n",
    "N_EPOCHS = 30        # por defecto 60; puedes reducir para debug\n",
    "BATCH_BASE = 128     # baseline\n",
    "NUM_WORKERS = 4\n",
    "N_SEEDS = 1          # =1 produce 45 modelos; set 3 to run 3 seeds per condition (much más caro)\n",
    "\n",
    "# Set a default seed for deterministic config ordering\n",
    "BASE_SEED = 42\n",
    "set_seed(BASE_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d10f32",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Dataset & Dataloaders** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b22729",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "train_transform_baseline = transforms.Compose([\n",
    "    transforms.RandomCrop(IMG_SIZE, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),(0.247,0.243,0.261))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),(0.247,0.243,0.261))\n",
    "])\n",
    "\n",
    "# We'll create a function to return loaders given batch_size and augmentation variant\n",
    "def make_dataloaders(batch_size=BATCH_BASE, augmentation='baseline'):\n",
    "    # download once\n",
    "    train_full = datasets.CIFAR10(root=str(DATA_DIR), train=True, download=True,\n",
    "                                  transform=(train_transform_baseline if augmentation=='baseline' else train_transform_baseline))\n",
    "    test = datasets.CIFAR10(root=str(DATA_DIR), train=False, download=True, transform=val_transform)\n",
    "\n",
    "    # 80/20 split of training into train/val as required\n",
    "    n = len(train_full)\n",
    "    n_train = int(0.8 * n)\n",
    "    n_val = n - n_train\n",
    "    train_ds, val_ds = random_split(train_full, [n_train, n_val], generator=torch.Generator().manual_seed(BASE_SEED))\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# quick check\n",
    "_train, _val, _test = make_dataloaders(batch_size=64)\n",
    "print(\"Sample counts -> train batches:\", len(_train), \"val batches:\", len(_val), \"test batches:\", len(_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d2add",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Definición de Modelos** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48d041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alexnet(num_classes=NUM_CLASSES, init_type='kaiming'):\n",
    "    model = models.alexnet(weights=None)\n",
    "    in_feat = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(in_feat, num_classes)\n",
    "    apply_init(model, init_type)\n",
    "    return model\n",
    "\n",
    "def get_vgg11(num_classes=NUM_CLASSES, init_type='kaiming'):\n",
    "    model = models.vgg11_bn(weights=None)\n",
    "    in_feat = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(in_feat, num_classes)\n",
    "    apply_init(model, init_type)\n",
    "    return model\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128,256,3,padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256*4*4, 512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256), nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def get_custom(init_type='kaiming'):\n",
    "    m = CustomCNN()\n",
    "    apply_init(m, init_type)\n",
    "    return m\n",
    "\n",
    "# initialization helper\n",
    "def apply_init(model, scheme='kaiming'):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            if scheme == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            elif scheme == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "            elif scheme == 'orthogonal':\n",
    "                nn.init.orthogonal_(m.weight)\n",
    "            else:\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if hasattr(m,'bias') and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "            if hasattr(m,'weight') and m.weight is not None:\n",
    "                nn.init.ones_(m.weight)\n",
    "            if hasattr(m,'bias') and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71aa87",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Entrenamiento y Validación** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47523c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_batch(output, target):\n",
    "    with torch.no_grad():\n",
    "        preds = torch.argmax(output, dim=1)\n",
    "        return (preds==target).float().mean().item()\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    n = 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        bs = imgs.size(0)\n",
    "        running_loss += loss.item()*bs\n",
    "        running_acc += accuracy_batch(out, labels)*bs\n",
    "        n += bs\n",
    "    return running_loss/n, running_acc/n\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device); labels = labels.to(device)\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            bs = imgs.size(0)\n",
    "            running_loss += loss.item()*bs\n",
    "            running_acc += accuracy_batch(out, labels)*bs\n",
    "            n += bs\n",
    "    return running_loss/n, running_acc/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3f423",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Learning Rate Scheduling** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scheduler(optimizer, schedule_name, epochs, warmup_epochs=5, base_lr=0.1, steps_per_epoch=None):\n",
    "    if schedule_name == 'onecycle':\n",
    "        if steps_per_epoch is None:\n",
    "            raise ValueError(\"steps_per_epoch required for OneCycle\")\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=base_lr, total_steps=epochs*steps_per_epoch)\n",
    "        return scheduler\n",
    "    elif schedule_name == 'cosine':\n",
    "        # we'll use LambdaLR with warmup\n",
    "        def lr_lambda(ep):\n",
    "            if ep < warmup_epochs:\n",
    "                return float(ep+1)/float(warmup_epochs)\n",
    "            # cosine annealing after warmup\n",
    "            t = (ep-warmup_epochs)/(epochs-warmup_epochs)\n",
    "            return 0.5*(1+np.cos(np.pi*t))\n",
    "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    elif schedule_name == 'step':\n",
    "        # step at 30 & 50 (if epochs > those)\n",
    "        def lr_lambda(ep):\n",
    "            if ep < warmup_epochs:\n",
    "                return float(ep+1)/float(warmup_epochs)\n",
    "            e = ep - warmup_epochs\n",
    "            if e < 25: return 1.0\n",
    "            elif e < 45: return 0.1\n",
    "            else: return 0.01\n",
    "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "    else:\n",
    "        # constant with warmup\n",
    "        def lr_lambda(ep):\n",
    "            return float(ep+1)/float(warmup_epochs) if ep < warmup_epochs else 1.0\n",
    "        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479db94",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Ejemplo de uso prueba** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_range_test(build_model_fn, train_loader, optimizer_cls, start_lr=1e-6, end_lr=1.0, iters=100):\n",
    "    model = build_model_fn().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer_cls(model.parameters(), lr=start_lr)\n",
    "    lrs = []\n",
    "    losses = []\n",
    "    itr = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device); labels = labels.to(device)\n",
    "        lr = start_lr * (end_lr/start_lr)**(itr/iters)\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, labels)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        lrs.append(lr); losses.append(loss.item())\n",
    "        itr += 1\n",
    "        if itr >= iters:\n",
    "            break\n",
    "    return lrs, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fa2b5",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Configuración de Experimentos** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344eaf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['alexnet','vgg','custom']\n",
    "AXIS = {}\n",
    "\n",
    "# Axis A: Optimizers (3 variants)\n",
    "AXIS['A'] = [\n",
    "    {'tag':'sgd',   'opt': {'name':'sgd',   'lr':0.1,  'weight_decay':5e-4, 'nesterov':False}},\n",
    "    {'tag':'sgd_n', 'opt': {'name':'sgd',   'lr':0.1,  'weight_decay':5e-4, 'nesterov':True}},\n",
    "    {'tag':'adamw', 'opt': {'name':'adamw', 'lr':1e-3, 'weight_decay':1e-2}}\n",
    "]\n",
    "\n",
    "# Axis B: Regularization (3 variants)\n",
    "AXIS['B'] = [\n",
    "    {'tag':'wd_1e-4',  'reg': {'weight_decay':1e-4, 'dropout':0.0, 'label_smoothing':0.0, 'mixup':False}},\n",
    "    {'tag':'dropout_0.2','reg': {'weight_decay':5e-4, 'dropout':0.2, 'label_smoothing':0.0, 'mixup':False}},\n",
    "    {'tag':'mixup_ls', 'reg': {'weight_decay':5e-4, 'dropout':0.0, 'label_smoothing':0.1, 'mixup':True, 'mixup_alpha':0.2}}\n",
    "]\n",
    "\n",
    "# Axis C: Batch sizes (3 variants)\n",
    "AXIS['C'] = [\n",
    "    {'tag':'bs32',  'batch_size':32},\n",
    "    {'tag':'bs128', 'batch_size':128},\n",
    "    {'tag':'bs512', 'batch_size':512}\n",
    "]\n",
    "\n",
    "# Axis D: LR schedules (3 variants)\n",
    "AXIS['D'] = [\n",
    "    {'tag':'const',  'schedule':{'name':'constant'}},\n",
    "    {'tag':'step',   'schedule':{'name':'step'}},\n",
    "    {'tag':'cosine', 'schedule':{'name':'cosine'}}\n",
    "]\n",
    "\n",
    "# Axis E: Initializations (3 variants)\n",
    "AXIS['E'] = [\n",
    "    {'tag':'kaiming', 'init':'kaiming'},\n",
    "    {'tag':'xavier',  'init':'xavier'},\n",
    "    {'tag':'orth',    'init':'orthogonal'}\n",
    "]\n",
    "\n",
    "# baseline config\n",
    "BASE = {\n",
    "    'init': 'kaiming',\n",
    "    'optimizer': {'name':'sgd','lr':0.1,'weight_decay':5e-4,'nesterov':False},\n",
    "    'schedule': {'name':'cosine'},\n",
    "    'batch_size': BATCH_BASE,\n",
    "    'reg': {'weight_decay':5e-4, 'dropout':0.0, 'label_smoothing':0.0, 'mixup':False},\n",
    "    'epochs': N_EPOCHS,\n",
    "    'warmup':5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a0e93",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Runner that executes one configuration (model+config) and saves history & checkpoint** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1654a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_by_name(name, init_type='kaiming'):\n",
    "    if name == 'alexnet':\n",
    "        return get_alexnet(init_type=init_type)\n",
    "    elif name == 'vgg':\n",
    "        return get_vgg11(init_type=init_type)\n",
    "    elif name == 'custom':\n",
    "        return get_custom(init_type=init_type)\n",
    "    else:\n",
    "        raise ValueError(name)\n",
    "\n",
    "def run_single(model_name, cfg, seed, out_prefix):\n",
    "    # set seed\n",
    "    set_seed(seed)\n",
    "\n",
    "    # dataloaders based on batch_size and augmentation (augmentation toggle in cfg['reg'] if needed)\n",
    "    bs = cfg.get('batch_size', BASE['batch_size'])\n",
    "    train_loader, val_loader, test_loader = make_dataloaders(batch_size=bs)\n",
    "\n",
    "    # build model\n",
    "    init_type = cfg.get('init', BASE['init'])\n",
    "    if model_name == 'alexnet':\n",
    "        model = get_alexnet(num_classes=NUM_CLASSES, init_type=init_type)\n",
    "    elif model_name == 'vgg':\n",
    "        model = get_vgg11(num_classes=NUM_CLASSES, init_type=init_type)\n",
    "    else:\n",
    "        model = get_custom(init_type=init_type)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # maybe apply dropout changes (for custom model we can change classifier dropout if needed — here we skip structural edits for simplicity)\n",
    "    # criterion (support label smoothing manually if >0)\n",
    "    label_smoothing = cfg.get('reg', {}).get('label_smoothing', 0.0)\n",
    "    if label_smoothing > 0:\n",
    "        # we'll use a custom loss wrapper\n",
    "        criterion = nn.CrossEntropyLoss()  # used in helper below when not smoothing; we'll implement smoothing in loop\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # optimizer\n",
    "    opt_cfg = cfg.get('optimizer', BASE['optimizer'])\n",
    "    if opt_cfg['name'] == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=opt_cfg['lr'], momentum=0.9, weight_decay=opt_cfg.get('weight_decay',0.0), nesterov=opt_cfg.get('nesterov',False))\n",
    "    elif opt_cfg['name'] == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=opt_cfg['lr'], weight_decay=opt_cfg.get('weight_decay',0.0))\n",
    "    elif opt_cfg['name'] == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=opt_cfg['lr'], weight_decay=opt_cfg.get('weight_decay',0.0))\n",
    "    else:\n",
    "        raise ValueError(opt_cfg['name'])\n",
    "\n",
    "    # scheduler\n",
    "    sched_cfg = cfg.get('schedule', BASE['schedule'])\n",
    "    scheduler = make_scheduler(optimizer, sched_cfg['name'], epochs=cfg['epochs'], warmup_epochs=cfg.get('warmup',5), base_lr=opt_cfg.get('lr',0.1), steps_per_epoch=len(train_loader))\n",
    "\n",
    "    # mixup\n",
    "    do_mixup = cfg.get('reg',{}).get('mixup', False)\n",
    "    mixup_alpha = cfg.get('reg',{}).get('mixup_alpha', 0.2)\n",
    "\n",
    "    # training loop\n",
    "    history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[], 'lr':[]}\n",
    "    best_val = -1\n",
    "    best_epoch = -1\n",
    "\n",
    "    for epoch in range(cfg['epochs']):\n",
    "        model.train()\n",
    "        train_loss = 0.0; train_acc = 0.0; n_train = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device); labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if do_mixup:\n",
    "                # simple mixup\n",
    "                lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
    "                idx = torch.randperm(imgs.size(0)).to(imgs.device)\n",
    "                imgs_mix = lam*imgs + (1-lam)*imgs[idx]\n",
    "                out = model(imgs_mix)\n",
    "                loss = lam*F.cross_entropy(out, labels) + (1-lam)*F.cross_entropy(out, labels[idx])\n",
    "            else:\n",
    "                out = model(imgs)\n",
    "                if label_smoothing > 0:\n",
    "                    # label smoothing: compute smoothed loss\n",
    "                    num_classes = out.size(1)\n",
    "                    log_probs = F.log_softmax(out, dim=1)\n",
    "                    with torch.no_grad():\n",
    "                        true_dist = torch.zeros_like(out)\n",
    "                        true_dist.fill_(label_smoothing/(num_classes-1))\n",
    "                        true_dist.scatter_(1, labels.data.unsqueeze(1), 1.0-label_smoothing)\n",
    "                    loss = torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
    "                else:\n",
    "                    loss = F.cross_entropy(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs_curr = imgs.size(0)\n",
    "            train_loss += loss.item()*bs_curr\n",
    "            train_acc += (out.argmax(dim=1)==labels).float().sum().item()\n",
    "            n_train += bs_curr\n",
    "\n",
    "            # onecycle step per batch if used\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "                scheduler.step()\n",
    "\n",
    "        train_loss /= n_train\n",
    "        train_acc /= n_train\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc = validate(model, val_loader, F.cross_entropy if label_smoothing==0 else criterion, device)\n",
    "\n",
    "        # scheduler step (if not OneCycle)\n",
    "        if not isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "\n",
    "        lr_now = optimizer.param_groups[0]['lr']\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(lr_now)\n",
    "\n",
    "        # save best\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            best_epoch = epoch\n",
    "            ckpt = {\n",
    "                'model_name': model_name,\n",
    "                'cfg': cfg,\n",
    "                'epoch': epoch,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'best_val': best_val\n",
    "            }\n",
    "            ckpt_path = CHECKPOINTS_DIR/f\"{out_prefix}_best.pth\"\n",
    "            torch.save(ckpt, ckpt_path)\n",
    "\n",
    "        print(f\"[{out_prefix}] Ep {epoch+1}/{cfg['epochs']}  tr_acc={train_acc:.4f} val_acc={val_acc:.4f} lr={lr_now:.6f}\")\n",
    "\n",
    "    # final save history\n",
    "    hist_path = HISTORY_DIR/f\"{out_prefix}_history.pkl\"\n",
    "    with open(hist_path, 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "\n",
    "    return {'best_val': best_val, 'best_epoch': best_epoch, 'history_path': str(hist_path)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c22368",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Orchestrator: build all configs and run them** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "\n",
    "# helper to merge base + axis variant config\n",
    "def merge_cfg(base, override):\n",
    "    cfg = dict(base)  # shallow copy\n",
    "    # deep-merge some fields\n",
    "    cfg.update({k:v for k,v in override.items() if k not in ['opt','reg','schedule','batch_size','init']})\n",
    "    if 'opt' in override:\n",
    "        cfg['optimizer'] = override['opt']\n",
    "    if 'reg' in override:\n",
    "        cfg['reg'] = override['reg']\n",
    "    if 'schedule' in override:\n",
    "        cfg['schedule'] = override['schedule']\n",
    "    if 'batch_size' in override:\n",
    "        cfg['batch_size'] = override['batch_size']\n",
    "    if 'init' in override:\n",
    "        cfg['init'] = override['init']\n",
    "    return cfg\n",
    "\n",
    "# Loop axes A..E\n",
    "axes_order = ['A','B','C','D','E']\n",
    "total_runs = 0\n",
    "for axis in axes_order:\n",
    "    variants = AXIS[axis]\n",
    "    for var in variants:\n",
    "        var_tag = var['tag']\n",
    "        # build cfg for this axis variant\n",
    "        cfg = dict(BASE)  # start from base\n",
    "        # overlay depending on axis\n",
    "        if axis == 'A':\n",
    "            cfg['optimizer'] = var['opt']\n",
    "        elif axis == 'B':\n",
    "            cfg['reg'] = var['reg']\n",
    "        elif axis == 'C':\n",
    "            cfg['batch_size'] = var['batch_size']\n",
    "        elif axis == 'D':\n",
    "            cfg['schedule'] = var['schedule']\n",
    "        elif axis == 'E':\n",
    "            cfg['init'] = var['init']\n",
    "        # run for each model\n",
    "        for model_name in MODELS:\n",
    "            for seed_idx in range(N_SEEDS):\n",
    "                seed = BASE_SEED + seed_idx + total_runs  # make deterministic varied seeds\n",
    "                run_tag = f\"{axis}_{var_tag}_{model_name}_s{seed_idx}\"\n",
    "                print(\"\\n===== RUN:\", run_tag, \"seed\", seed, \"=====\")\n",
    "                out = run_single(model_name, cfg, seed, out_prefix=run_tag)\n",
    "                summary_rows.append({\n",
    "                    'axis': axis,\n",
    "                    'variant': var_tag,\n",
    "                    'model': model_name,\n",
    "                    'seed_index': seed_idx,\n",
    "                    'seed': seed,\n",
    "                    'best_val': out['best_val'],\n",
    "                    'best_epoch': out['best_epoch'],\n",
    "                    'history_path': out['history_path']\n",
    "                })\n",
    "                total_runs += 1\n",
    "\n",
    "# save summary CSV\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_summary.to_csv(RESULTS_DIR/'summary_runs.csv', index=False)\n",
    "print(\"All runs finished. Summary saved to\", RESULTS_DIR/'summary_runs.csv')\n",
    "print(\"Total runs:\", total_runs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc0d3f",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Graficar resultados** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a13c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RESULTS_DIR/'summary_runs.csv')\n",
    "agg = df.groupby(['axis','variant','model']).agg(mean_val=('best_val','mean'), std_val=('best_val','std'), n=('best_val','count')).reset_index()\n",
    "display(agg)\n",
    "agg.to_csv(RESULTS_DIR/'summary_agg.csv', index=False)\n",
    "\n",
    "# Example plot: best_val per model for a chosen axis\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "for axis in axes_order:\n",
    "    sub = agg[agg['axis']==axis]\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.barplot(data=sub, x='variant', y='mean_val', hue='model')\n",
    "    plt.title(f'Axis {axis} — mean validation acc (best epoch)')\n",
    "    plt.ylim(0,1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e3784",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Resultados** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9458371f",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Análisis** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf21b73",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Limitaciones** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23cab6",
   "metadata": {},
   "source": [
    "### <font color= #d4b1e6> **Referencias** </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iteso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
